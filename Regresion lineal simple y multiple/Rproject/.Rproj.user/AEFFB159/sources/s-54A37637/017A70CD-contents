---
title: "Exploratory Text Analysis Report"
author: "Yaniela Fernandez M."
date: "10/15/2020"
output:
  html_document:
    toc: true
    theme: united
---


## Summary

This report describes the process to load, pre-process, and explore a dataset of text documents using the ```tm``` package in the R programming language. The main goal of this project is to prepare these documents as a training set for a text predicting model. After removing frequent words, numbers, and punctuation, it calculates the frequency of single words, pairs of words (two-grams), and word triples (three-grams) in the documents.

# Source data

The data for this project comes from a collection of corpora for various languages that are collected from publicly available sources by a web crawler. The dataset was download from the Coursera site at this address: https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip

The downloaded zip file is composed of four sets of three files:

   - lan_loc.blogs.txt: text obtained from blogs
   - lan_loc.news.txt: text obtained from news feeds
   - lan_loc.twitter.txt: text obtained from Twitter

where lan_loc denotes language and locale and is either ```de_DE``` (German), ```en_US``` (English, US), ```fi_FI``` (Finnish) or ```ru_RU``` (Russian). **This project only uses the English language files**.
 
# Selecting the data sample

The data files are relatively large (between 200 and 319 MB), from about 900,000 lines of text in the blog's file to over 2 million in the Twitter file. 

```{r echo=FALSE, message=FALSE, warning=FALSE}
library(tm)
library(RWeka)
library(tokenizers)
library(ggplot2)
library(wordcloud)
library(RColorBrewer)
library(formattable)

blogs <- readLines("SwiftKeyDataset/en_US.blogs.txt", warn=FALSE, encoding="UTF-8")
twits <- readLines("SwiftKeyDataset/en_US.twitter.txt", warn=FALSE, encoding="UTF-8")
news <- readLines("SwiftKeyDataset/en_US.news.txt", warn=FALSE, encoding="UTF-8")


len_blogs <- length(blogs)
len_twits <- length(twits)
len_news <- length(news)

size_blogs <- object.size(blogs)
size_twits <- object.size(twits)
size_news <- object.size(news)

max_line_blogs <- max(nchar(blogs))
max_line_twits <- max(nchar(twits))
max_line_news <- max(nchar(news))

avg_line_blogs <- mean(nchar(blogs))
avg_line_twits <- mean(nchar(twits))
avg_line_news <- mean(nchar(news))

# Summary table

data_metrics <- data.frame(file_name = c("en_US.blogs.txt","en_US.news.txt","en_US.twitter.txt"),
                           size = c(format(size_blogs, units = "auto"), 
                                    format(size_news, units = "auto"), 
                                    format(size_twits, units = "auto")),
                           lines = c(format(len_blogs, big.mark=","),
                                     format(len_news, big.mark=","),
                                     format(len_twits, big.mark=",")),
                           Average_line_length = c(round(avg_line_blogs,0), 
                                                   round(avg_line_news,0), 
                                                   round(avg_line_twits,0)),
                           max_line_length = c(format(max_line_blogs, big.mark=","), 
                                               format(max_line_news, big.mark=","), 
                                               format(max_line_twits, big.mark=","))
)

colnames(data_metrics) <- c('File Name', 'File Size', 'Number of Lines', 'Average Length of Lines', 'Maximun Length of a line') 
formattable(data_metrics)



```


To speed up data exploration, it was used smaller samples from each of the files. **In this report, for illustration purposes, it's used a random sample of text extracted from the three files** (1 percent of the total number of lines on each file). Data files **en_US.blogs.txt**,**en_US.twitter.txt** and **en_US.news.txt**  are loaded on ```blogs ``` , ```twits```, and ```news``` variables respectively.  

```{r message=FALSE, warning=FALSE}
set.seed(4532)

twits_sample <- sample(twits, length(twits)*.01)
news_sample <- sample(news, length(news)*.01)
blogs_sample <- sample(blogs, length(blogs)*.01)

combined_sample <- c(twits_sample, blogs_sample, news_sample)
combined_sample <- iconv(combined_sample, "UTF-8","ASCII", sub="")
combined_sample <- gsub("\\b(\\w+) \\1\\b", "\\1" , combined_sample)
combined_sample <- gsub("(.)\\1{2,}" ,"\\1", combined_sample)

# Release memory
rm(twits,news, blogs,twits_sample,news_sample, blogs_sample)
```

# Data preprocessing 

A Corpus (collection of documents) is created based on the three samples. 

```{r message=FALSE, warning=FALSE}
corpus <- VCorpus(VectorSource(combined_sample))
print(corpus)
```

The result is a structure of type VCorpus (‘virtual corpus’ or loaded into memory) with 33,365 documents (each line of text in the source is loaded as a document in the corpus). One thing I notice at this stage is that the text file ```combined_sample```, when loaded into R, occupies 6.2 MB whereas the associated VCorpus object is much larger, at 138.4 MB.

Common data cleaning tasks associated with text mining are:

- Converting the entire document to lower case
- Removing punctuation marks (periods, commas, hyphens, etc)
- Removing stopwords (common words such as: “and”, “or”, “not”, “in”, “is”, etc)
- Removing numbers
- Removing extra whitespace

The ```tm``` package provides several functions to carry out these tasks, which are applied to the document collection as transformations via the ```tm_map()``` function.

```{r message=FALSE, warning=FALSE}
corpus <- tm_map(corpus, tolower)
corpus <- tm_map(corpus, removePunctuation, mc.cores=1)
corpus <- tm_map(corpus, removeNumbers, mc.cores=1)
corpus <- tm_map(corpus, PlainTextDocument, mc.cores=1)
corpus <- tm_map(corpus, removeWords, stopwords("english"))
corpus <- tm_map(corpus, stripWhitespace)
```

# Data exploration

A common approach in text mining is to create a term-document matrix from a corpus: elements in this matrix represent the occurrence of a term (a word, or an n-gram) in a document of the corpus. The NGramTokenizer() function from the RWeka package it`s used to construct the matrices of unigrams, bigrams, and trigrams.

```{r message=FALSE, warning=FALSE}
# 1-gram
unigram <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1))
unigram_tdm <- TermDocumentMatrix(corpus, control = list(tokenize = unigram))

# 2-gram
bigram <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
bigram_tdm <- TermDocumentMatrix(corpus, control = list(tokenize = bigram))

# 3-gram
trigram <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
trigram_tdm <- TermDocumentMatrix(corpus, control = list(tokenize = trigram))
```

A problem with those matrices is that they tend to get very big, and they are extremely sparse. The matrix created for **1-gram** with the above command has only 345,587 non-zero elements out of over 1 billion (less than 0.03 percent). 

```{r message=FALSE, warning=FALSE}
unigram_tdm 
```

Due to memory constraints, the sparse terms are removed in the previous matrix.    

```{r message=FALSE, warning=FALSE}
uni_tdm_sparse <- removeSparseTerms(unigram_tdm, sparse=0.99)
```

The plots below show the histograms of the ten most frequent words in the corpus for 1-gram and 3-grams matrix. The most frequent word and two-words are “just” and "cant wait see" respectively.

```{r echo=FALSE, message=FALSE, warning=FALSE}
 unigram_freqTerm <- findFreqTerms(uni_tdm_sparse,lowfreq = 40)
 unigram_freq <- rowSums(as.matrix(uni_tdm_sparse[unigram_freqTerm,]))
 unigram_ord <- order(unigram_freq, decreasing = TRUE)
 unigram_freq <- data.frame(word=names(unigram_freq[unigram_ord]), frequency=unigram_freq[unigram_ord])


 bigram_freqTerm <- findFreqTerms(bigram_tdm,lowfreq = 40)
 bigram_freq <- rowSums(as.matrix(bigram_tdm[bigram_freqTerm,]))
 bigram_ord <- order(bigram_freq, decreasing = TRUE)
 bigram_freq <- data.frame(word=names(bigram_freq[bigram_ord]), frequency=bigram_freq[bigram_ord])

 trigram_freqTerm <- findFreqTerms(trigram_tdm,lowfreq=10)
 trigram_freq <- rowSums(as.matrix(trigram_tdm[trigram_freqTerm,]))
 trigram_ord <- order(trigram_freq, decreasing = TRUE)
 trigram_freq <- data.frame(word=names(trigram_freq[trigram_ord]), frequency=trigram_freq[trigram_ord])


library(ggpubr)

g1<-ggplot(unigram_freq[1:10,], aes(factor(word, levels = unique(word)), frequency)) +
  geom_bar(stat = 'identity',width=0.5, fill="tomato2")+
  theme(axis.text.x=element_text(angle=90))+
  xlab('1-gram')+
  ylab('Frequency')

g3<-ggplot(trigram_freq, aes(factor(word, levels = unique(word)), frequency)) +
  geom_bar(stat = 'identity',width=0.5, fill="tomato2")+ theme(axis.text.x=element_text(angle=90))+
  xlab('3-grams')+  ylab('Frequency')

ggarrange(g1, g3, ncol = 2, nrow = 1)

```

In the next plot, the most frequent 2-grams are “cant wait”, which is consistent with the most frequent 2-grams. Notice that “last night” is in third place, and a "last year" in sixteenth place. Based on these results, one could imagine a scenario where if a user inputs “last”, the model predicts the most likely completion as “night”, followed by “year” (we would like the application to output more than one suggestion for the user to choose from, so the result should be a ranking of the 5-10 most likely terms).

```{r echo=FALSE, message=FALSE, warning=FALSE}

ggplot(bigram_freq[1:20,], aes(factor(word, levels = unique(word)), frequency)) +
  geom_bar(stat = 'identity',width=0.5, fill="tomato2")+
  theme(axis.text.x=element_text(angle=90))+
  xlab('2-grams')+
  ylab('Frequency')
  
```

As an alternative to the previous plots, the following graph shows word clouds for the most frequent 2-grams.

```{r echo=FALSE, message=FALSE, warning=FALSE}
wordcloud(bigram_freq$word, bigram_freq$frequency, max.words=30, colors=brewer.pal(7,"YlOrRd"))
```

# Findings

The longer the N-gram, the less frequent they become. Careful cleaning is necessary, there are some Spanish words that need to be removed to avoid grammatical misunderstandings.

# Next steps

Only one percent of the data were sampled for this analysis. It would be interesting to get a wider sample for the prediction algorithm (75% training / 25% testing), but there are some memory restrictions to be aware of.  

The idea for predicting text would be to generate 2-grams,  3-grams, and 4-grams matrices with the frequencies of different combinations. Next,  the word (or group of words) enter by the user input will match with the most probable (n+1)-gram. A dictionary will be built to improve the matching process

The source code is available at https://github.com/yaniela/DataSciencieCapstoneProject
